{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3e39063",
   "metadata": {},
   "source": [
    "# ADS 509 Module 3: Group Comparison \n",
    "\n",
    "## Renetta Nelson\n",
    "\n",
    "## May 29, 2023\n",
    "\n",
    "The task of comparing two groups of text is fundamental to textual analysis. There are innumerable applications: survey respondents from different segments of customers, speeches by different political parties, words used in Tweets by different constituencies, etc. In this assignment you will build code to effect comparisons between groups of text data, using the ideas learned in reading and lecture.\n",
    "\n",
    "This assignment asks you to analyze the lyrics and Twitter descriptions for the two artists you selected in Module 1. If the results from that pull were not to your liking, you are welcome to use the zipped data from the “Assignment Materials” section. Specifically, you are asked to do the following: \n",
    "\n",
    "* Read in the data, normalize the text, and tokenize it. When you tokenize your Twitter descriptions, keep hashtags and emojis in your token set. \n",
    "* Calculate descriptive statistics on the two sets of lyrics and compare the results. \n",
    "* For each of the four corpora, find the words that are unique to that corpus. \n",
    "* Build word clouds for all four corpora. \n",
    "\n",
    "Each one of the analyses has a section dedicated to it below. Before beginning the analysis there is a section for you to read in the data and do your cleaning (tokenization and normalization). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "abe420bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a9f064bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this space for any additional import statements you need\n",
    "\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bcbe6342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place any addtional functions or constants you need here. \n",
    "\n",
    "# Some punctuation variations\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "\n",
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "# Two useful regex\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "hashtag_pattern = re.compile(r\"^#[0-9a-zA-Z]+\")\n",
    "\n",
    "# It's handy to have a full set of emojis\n",
    "all_language_emojis = set()\n",
    "\n",
    "for country in emoji.EMOJI_DATA : \n",
    "    for em in emoji.EMOJI_DATA[country] : \n",
    "        all_language_emojis.add(em)\n",
    "\n",
    "# and now our functions\n",
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity, and num_tokens most common\n",
    "        tokens. Return a list of \n",
    "    \"\"\"\n",
    "\n",
    "    # Place your Module 2 solution here\n",
    "    \n",
    "    num_characters = 0\n",
    "    for i in tokens:\n",
    "        num_characters = num_characters + len(i)\n",
    "\n",
    "\n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    lexical_diversity = num_unique_tokens / num_tokens\n",
    "    num_characters = num_characters #len(list(tokens)) #(nltk.FreqDist(nltk.Text(tokens))).N()\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "\n",
    "\n",
    "    \n",
    "def contains_emoji(s):\n",
    "    \n",
    "    s = str(s)\n",
    "    emojis = [ch for ch in s if emoji.is_emoji(ch)]\n",
    "\n",
    "    return(len(emojis) > 0)\n",
    "\n",
    "\n",
    "def remove_stop(tokens) :\n",
    "    # modify this function to remove stopwords\n",
    "\n",
    "    #return (t for t in tokens if t.lower() not in stopwords)\n",
    "    return(tokens)\n",
    " \n",
    "def remove_punctuation(text, punct_set=tw_punct) : \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) : \n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That \n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "    \n",
    "    # modify this function to return tokens\n",
    "    #for i in text:\n",
    "    #    text = i.split(\" \")\n",
    "\n",
    "    return(text)\n",
    "\n",
    "def prepare(text, pipeline) : \n",
    "    tokens = str(text)\n",
    "    \n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)\n",
    "        \n",
    "    return(tokens)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47735524",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Use this section to ingest your data into the data structures you plan to use. Typically this will be a dictionary or a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ff88201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel fre to use the below cells as an example or read in the data in a way you prefer\n",
    "\n",
    "data_location = \"C:/Users/nelso/Desktop/\" # change to your location if it is not in the same directory as your notebook\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\"\n",
    "\n",
    "lyric_artist1 = \"cher/\"\n",
    "\n",
    "artist_files = {'cher':'cher_followers_data.txt',\n",
    "                'robyn':'robynkonichiwa_followers_data.txt'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "df415d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = pd.read_csv(data_location + twitter_folder + artist_files['cher'],\n",
    "                           sep=\"\\t\",\n",
    "                           quoting=3)\n",
    "\n",
    "twitter_data['artist'] = \"cher\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "966804cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data_2 = pd.read_csv(data_location + twitter_folder + artist_files['robyn'],\n",
    "                             sep=\"\\t\",\n",
    "                             quoting=3)\n",
    "twitter_data_2['artist'] = \"robyn\"\n",
    "\n",
    "twitter_data = pd.concat([\n",
    "    twitter_data,twitter_data_2])\n",
    "    \n",
    "del(twitter_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "674767d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the lyrics here\n",
    "\n",
    "# Create dictionary for lyrics data\n",
    "lyrics_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "artist_names = []\n",
    "song_names = []\n",
    "lyrics_songs =[]\n",
    "\n",
    "\n",
    "for artists in os.listdir(data_location + lyrics_folder):\n",
    "    for lyrics in os.listdir(data_location + lyrics_folder + artists):\n",
    "        artist, song_name = lyrics.split(\"_\")\n",
    "\n",
    "        song_name = song_name.replace(\".txt\", \" \")\n",
    "        artist_names.append(artist)\n",
    "        song_names.append(song_name)\n",
    "\n",
    "\n",
    "        with open(data_location + lyrics_folder + artists + '/' + lyrics) as infile:\n",
    "            next(infile)\n",
    "            next(infile)\n",
    "            next(infile)\n",
    "            next(infile)\n",
    "\n",
    "            lyrics_songs.append(infile.read())\n",
    "\n",
    "\n",
    "lyrics_data[\"artist\"] = artist_names\n",
    "lyrics_data[\"s_names\"] = song_names\n",
    "lyrics_data[\"lyrics\"] = lyrics_songs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9892d14",
   "metadata": {},
   "source": [
    "## Tokenization and Normalization\n",
    "\n",
    "In this next section, tokenize and normalize your data. We recommend the following cleaning. \n",
    "\n",
    "**Lyrics** \n",
    "\n",
    "* Remove song titles\n",
    "* Casefold to lowercase\n",
    "* Remove stopwords (optional)\n",
    "* Remove punctuation\n",
    "* Split on whitespace\n",
    "\n",
    "Removal of stopwords is up to you. Your descriptive statistic comparison will be different if you include stopwords, though TF-IDF should still find interesting features for you. Note that we remove stopwords before removing punctuation because the stopword set includes punctuation.\n",
    "\n",
    "**Twitter Descriptions** \n",
    "\n",
    "* Casefold to lowercase\n",
    "* Remove stopwords\n",
    "* Remove punctuation other than emojis or hashtags\n",
    "* Split on whitespace\n",
    "\n",
    "Removing stopwords seems sensible for the Twitter description data. Remember to leave in emojis and hashtags, since you analyze those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5ca379eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the `pipeline` techniques from BTAP Ch 1 or 5\n",
    "\n",
    "my_pipeline = [str.lower, remove_punctuation, tokenize, remove_stop]\n",
    "\n",
    "lyrics_data[\"tokens\"] = lyrics_data[\"lyrics\"].apply(prepare,pipeline=my_pipeline)\n",
    "lyrics_data[\"num_tokens\"] = lyrics_data[\"tokens\"].map(len) \n",
    "\n",
    "twitter_data[\"tokens\"] = twitter_data[\"description\"].apply(prepare,pipeline=my_pipeline)\n",
    "twitter_data[\"num_tokens\"] = twitter_data[\"tokens\"].map(len) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6cf534be",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data['has_emoji'] = twitter_data[\"description\"].apply(contains_emoji)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ec69ac9",
   "metadata": {},
   "source": [
    "Let's take a quick look at some descriptions with emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0a5a0512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>description</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>557474</th>\n",
       "      <td>cher</td>\n",
       "      <td>Events Producer 💪🏾 Ngaarr #Gomeroi #Dhanggati ...</td>\n",
       "      <td>events producer 💪🏾 ngaarr #gomeroi #dhanggati ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458758</th>\n",
       "      <td>cher</td>\n",
       "      <td>sempre bella, Inamorata della vita 💃🕺 felice😍💞...</td>\n",
       "      <td>sempre bella inamorata della vita 💃🕺 felice😍💞 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3969388</th>\n",
       "      <td>cher</td>\n",
       "      <td>✨ Creativo 🇲🇽 Mexicano 🌎 Monterrey, NL.</td>\n",
       "      <td>✨ creativo 🇲🇽 mexicano 🌎 monterrey nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651934</th>\n",
       "      <td>cher</td>\n",
       "      <td>🙃</td>\n",
       "      <td>🙃</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18997</th>\n",
       "      <td>robyn</td>\n",
       "      <td>copywriter + independent beckett scholar, @gol...</td>\n",
       "      <td>copywriter  independent beckett scholar goldsm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3691420</th>\n",
       "      <td>cher</td>\n",
       "      <td>Mother of one beautiful 6 year old girl, Jaidi...</td>\n",
       "      <td>mother of one beautiful 6 year old girl jaidin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992573</th>\n",
       "      <td>cher</td>\n",
       "      <td>Aries♈</td>\n",
       "      <td>aries♈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462421</th>\n",
       "      <td>cher</td>\n",
       "      <td>she/her 🤸‍♀️✨ pm of tls @ cloudflare... major ...</td>\n",
       "      <td>sheher 🤸‍♀️✨ pm of tls  cloudflare major key a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3720594</th>\n",
       "      <td>cher</td>\n",
       "      <td>Paulista/Paulistano em Carapicuíba/SP 🇧🇷 2.7 🎉...</td>\n",
       "      <td>paulistapaulistano em carapicuíbasp 🇧🇷 27 🎉 vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420789</th>\n",
       "      <td>cher</td>\n",
       "      <td>26♈ Ele/Dele 🏳️‍🌈💬💤🎶💻🎬🎮</td>\n",
       "      <td>26♈ eledele 🏳️‍🌈💬💤🎶💻🎬🎮</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        artist                                        description  \\\n",
       "557474    cher  Events Producer 💪🏾 Ngaarr #Gomeroi #Dhanggati ...   \n",
       "458758    cher  sempre bella, Inamorata della vita 💃🕺 felice😍💞...   \n",
       "3969388   cher            ✨ Creativo 🇲🇽 Mexicano 🌎 Monterrey, NL.   \n",
       "651934    cher                                                  🙃   \n",
       "18997    robyn  copywriter + independent beckett scholar, @gol...   \n",
       "3691420   cher  Mother of one beautiful 6 year old girl, Jaidi...   \n",
       "992573    cher                                             Aries♈   \n",
       "462421    cher  she/her 🤸‍♀️✨ pm of tls @ cloudflare... major ...   \n",
       "3720594   cher  Paulista/Paulistano em Carapicuíba/SP 🇧🇷 2.7 🎉...   \n",
       "420789    cher                            26♈ Ele/Dele 🏳️‍🌈💬💤🎶💻🎬🎮   \n",
       "\n",
       "                                                    tokens  \n",
       "557474   events producer 💪🏾 ngaarr #gomeroi #dhanggati ...  \n",
       "458758   sempre bella inamorata della vita 💃🕺 felice😍💞 ...  \n",
       "3969388              ✨ creativo 🇲🇽 mexicano 🌎 monterrey nl  \n",
       "651934                                                   🙃  \n",
       "18997    copywriter  independent beckett scholar goldsm...  \n",
       "3691420  mother of one beautiful 6 year old girl jaidin...  \n",
       "992573                                              aries♈  \n",
       "462421   sheher 🤸‍♀️✨ pm of tls  cloudflare major key a...  \n",
       "3720594  paulistapaulistano em carapicuíbasp 🇧🇷 27 🎉 vi...  \n",
       "420789                              26♈ eledele 🏳️‍🌈💬💤🎶💻🎬🎮  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data[twitter_data.has_emoji].sample(10)[[\"artist\",\"description\",\"tokens\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b2c55c9",
   "metadata": {},
   "source": [
    "With the data processed, we can now start work on the assignment questions. \n",
    "\n",
    "Q: What is one area of improvement to your tokenization that you could theoretically carry out? (No need to actually do it; let's not make perfect the enemy of good enough.)\n",
    "\n",
    "A: For the Twitter data, a Tweet tokenizer could be carried out. By implementing this tokenizer, emojis can be considered different words as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1594271",
   "metadata": {},
   "source": [
    "## Calculate descriptive statistics on the two sets of lyrics and compare the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "dc25e937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Statistics for Cher Lyrics\n",
      "\n",
      "There are 316 tokens in the data.\n",
      "There are 316 unique tokens in the data.\n",
      "There are 341088 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "\n",
      "\n",
      "\n",
      "Descriptive Statistics for Robyn Lyrics\n",
      "\n",
      "There are 104 tokens in the data.\n",
      "There are 95 unique tokens in the data.\n",
      "There are 145840 characters in the data.\n",
      "The lexical diversity is 0.913 in the data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[104, 95, 0.9134615384615384, 145840]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "print(\"Descriptive Statistics for Cher Lyrics\\n\")\n",
    "\n",
    "descriptive_stats(lyrics_data[\"lyrics\"].loc[lyrics_data['artist'] == 'cher'], verbose=True)\n",
    "descriptive_stats(lyrics_data[\"lyrics\"].loc[lyrics_data['artist'] == 'cher'], verbose=False)\n",
    "descriptive_stats(lyrics_data[\"lyrics\"].loc[lyrics_data['artist'] == 'cher'], verbose=False)\n",
    "descriptive_stats(lyrics_data[\"lyrics\"].loc[lyrics_data['artist'] == 'cher'], verbose=False)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Descriptive Statistics for Robyn Lyrics\\n\")\n",
    "\n",
    "\n",
    "descriptive_stats(lyrics_data[\"lyrics\"].loc[lyrics_data['artist'] == 'robyn'], verbose=True)\n",
    "descriptive_stats(lyrics_data[\"lyrics\"].loc[lyrics_data['artist'] == 'robyn'], verbose=False)\n",
    "descriptive_stats(lyrics_data[\"lyrics\"].loc[lyrics_data['artist'] == 'robyn'], verbose=False)\n",
    "descriptive_stats(lyrics_data[\"lyrics\"].loc[lyrics_data['artist'] == 'robyn'], verbose=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67a2ada9",
   "metadata": {},
   "source": [
    "Q: what observations do you make about these data? \n",
    "\n",
    "A: The first thing I noticed was the difference between the tokens and unique tokens for each set of lyrics. The tokens and unique tokens are equal for the cher lyrics; however for the robyn lyrics you see a slight difference in the amount. There are more characters in the cher lyrics than in the robyn lyrics. Due to these differences, the lexical diversity is different as well.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "750aa526",
   "metadata": {},
   "source": [
    "## Find tokens uniquely related to a corpus\n",
    "\n",
    "Typically we would use TF-IDF to find unique tokens in documents. Unfortunately, we either have too few documents (if we view each data source as a single document) or too many (if we view each description as a separate document). In the latter case, our problem will be that descriptions tend to be short, so our matrix would be too sparse to support analysis. \n",
    "\n",
    "To avoid these problems, we will create a custom statistic to identify words that are uniquely related to each corpus. The idea is to find words that occur often in one corpus and infrequently in the other(s). Since corpora can be of different lengths, we will focus on the _concentration_ of tokens within a corpus. \"Concentration\" is simply the count of the token divided by the total corpus length. For instance, if a corpus had length 100,000 and a word appeared 1,000 times, then the concentration would be $\\frac{1000}{100000} = 0.01$. If the same token had a concentration of $0.005$ in another corpus, then the concentration ratio would be $\\frac{0.01}{0.005} = 2$. Very rare words can easily create infinite ratios, so you will also add a cutoff to your code so that a token must appear at least $n$ times for you to return it. \n",
    "\n",
    "An example of these calculations can be found in [this spreadsheet](https://docs.google.com/spreadsheets/d/1P87fkyslJhqXFnfYezNYrDrXp_GS8gwSATsZymv-9ms). Please don't hesitate to ask questions if this is confusing. \n",
    "\n",
    "In this section find 10 tokens for each of your four corpora that meet the following criteria: \n",
    "\n",
    "1. The token appears at least `n` times in all corpora\n",
    "1. The tokens are in the top 10 for the highest ratio of appearances in a given corpora vs appearances in other corpora.\n",
    "\n",
    "You will choose a cutoff for yourself based on the side of the corpus you're working with. If you're working with the Robyn-Cher corpora provided, `n=5` seems to perform reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ce72f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpora 1         freq\n",
      "token       \n",
      "       58913\n",
      "e      31202\n",
      "o      25328\n",
      "t      21654\n",
      "a      19322\n",
      "Corpora 2         freq\n",
      "token       \n",
      "       24515\n",
      "e      12523\n",
      "o      10394\n",
      "t      10113\n",
      "i       8262\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "new_token1 = []\n",
    "\n",
    "\n",
    "#Initialize corporas\n",
    "\n",
    "corpora1 = lyrics_data.loc[lyrics_data['artist'] == 'cher']\n",
    "corpora2 = lyrics_data.loc[lyrics_data['artist'] == 'robyn']\n",
    "\n",
    "\n",
    "\n",
    "#Count total number in corpora\n",
    "\n",
    "tot_corp1 = len(corpora1)\n",
    "#print(\"Total Corpus Length (Corpora 1): \", tot_corp1)\n",
    "\n",
    "tot_corp2 = len(corpora2)\n",
    "#print(\"Total Corpus Length (Corpora 2): \", tot_corp2)\n",
    "\n",
    "\n",
    "#def get_ratio(word, fd_corpus_1, fd_corpus_2, len_1, len_2):\n",
    "#   frac_1 = get_word_frac (word, corpora1, tot_corp1)\n",
    "#   frac_2 = get_Word_frac(word, corpora2, tot_corp2)#\n",
    "\n",
    "#   if fac_2 > 0:\n",
    "#       return(frac_1 / frac_2)\n",
    "\n",
    "#   else: return(float('NaN'))\n",
    "\n",
    "\n",
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "\n",
    "    # process tokens and update counter\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # create counter and run through all data\n",
    "    counter = Counter()\n",
    "    df[column].map(update)\n",
    "\n",
    "    # transform counter into data frame\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    \n",
    "    return freq_df.sort_values('freq', ascending=False)\n",
    "\n",
    "\n",
    "corp1_count = lyrics_data[\"num_tokens\"].loc[lyrics_data['artist'] == 'cher']\n",
    "corp2_count = lyrics_data[\"num_tokens\"].loc[lyrics_data['artist'] == 'robyn']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "corp1 = count_words(corpora1)\n",
    "print(\"Corpora 1\", corp1.head(5))\n",
    "corp2 = count_words(corpora2)\n",
    "print(\"Corpora 2\", corp2.head(5))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53526fcd",
   "metadata": {},
   "source": [
    "Q: What are some observations about the top tokens? Do you notice any interesting items on the list? \n",
    "\n",
    "A: I was not able to run the code using the examples in the book. I kept getting an error. I was able to look through the lyric data using the definition provided below. I noticed that three common tokens were e, t, and o. These had the highest frequencies for lyrics of both cher and robyn. I found blanks on the list. I am not sure if I forgot to account for something or if the blanks were supposed to be there. So I found that pretty interesting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f4f52b3",
   "metadata": {},
   "source": [
    "## Build word clouds for all four corpora. \n",
    "\n",
    "For building wordclouds, we'll follow exactly the code of the text. The code in this section can be found [here](https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/ch01/First_Insights.ipynb). If you haven't already, you should absolutely clone the repository that accompanies the book. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "786b2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n",
    "\n",
    "    wc = WordCloud(width=800, height=400, \n",
    "                   background_color= \"black\", colormap=\"Paired\", \n",
    "                   max_font_size=150, max_words=max_words)\n",
    "    \n",
    "    # convert data frame into dict\n",
    "    if type(word_freq) == pd.Series:\n",
    "        counter = Counter(word_freq.fillna(0).to_dict())\n",
    "    else:\n",
    "        counter = word_freq\n",
    "\n",
    "    # filter stop words in frequency counter\n",
    "    if stopwords is not None:\n",
    "        counter = {token:freq for (token, freq) in counter.items() \n",
    "                              if token not in stopwords}\n",
    "    wc.generate_from_frequencies(counter)\n",
    " \n",
    "    plt.title(title) \n",
    "\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    \n",
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "\n",
    "    # process tokens and update counter\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # create counter and run through all data\n",
    "    counter = Counter()\n",
    "    df[column].map(update)\n",
    "\n",
    "    # transform counter into data frame\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    \n",
    "    return freq_df.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2d5ee6ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only supported for TrueType fonts",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[180], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m corp3 \u001b[39m=\u001b[39m count_words(corpora3)\n\u001b[0;32m     10\u001b[0m corp4 \u001b[39m=\u001b[39m count_words(corpora4)\n\u001b[1;32m---> 14\u001b[0m word_corp1 \u001b[39m=\u001b[39m wordcloud(corp1[\u001b[39m\"\u001b[39;49m\u001b[39mfreq\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWord Cloud for Corpora 1 \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(word_corp1)\n",
      "Cell \u001b[1;32mIn[171], line 19\u001b[0m, in \u001b[0;36mwordcloud\u001b[1;34m(word_freq, title, max_words, stopwords)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mif\u001b[39;00m stopwords \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     counter \u001b[39m=\u001b[39m {token:freq \u001b[39mfor\u001b[39;00m (token, freq) \u001b[39min\u001b[39;00m counter\u001b[39m.\u001b[39mitems() \n\u001b[0;32m     18\u001b[0m                           \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords}\n\u001b[1;32m---> 19\u001b[0m wc\u001b[39m.\u001b[39;49mgenerate_from_frequencies(counter)\n\u001b[0;32m     21\u001b[0m plt\u001b[39m.\u001b[39mtitle(title) \n\u001b[0;32m     23\u001b[0m plt\u001b[39m.\u001b[39mimshow(wc, interpolation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nelso\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\wordcloud\\wordcloud.py:508\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    505\u001b[0m transposed_font \u001b[39m=\u001b[39m ImageFont\u001b[39m.\u001b[39mTransposedFont(\n\u001b[0;32m    506\u001b[0m     font, orientation\u001b[39m=\u001b[39morientation)\n\u001b[0;32m    507\u001b[0m \u001b[39m# get size of resulting text\u001b[39;00m\n\u001b[1;32m--> 508\u001b[0m box_size \u001b[39m=\u001b[39m draw\u001b[39m.\u001b[39;49mtextbbox((\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m), word, font\u001b[39m=\u001b[39;49mtransposed_font, anchor\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    509\u001b[0m \u001b[39m# find possible places using integral image:\u001b[39;00m\n\u001b[0;32m    510\u001b[0m result \u001b[39m=\u001b[39m occupancy\u001b[39m.\u001b[39msample_position(box_size[\u001b[39m3\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmargin,\n\u001b[0;32m    511\u001b[0m                                    box_size[\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmargin,\n\u001b[0;32m    512\u001b[0m                                    random_state)\n",
      "File \u001b[1;32mc:\\Users\\nelso\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\PIL\\ImageDraw.py:671\u001b[0m, in \u001b[0;36mImageDraw.textbbox\u001b[1;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[0;32m    669\u001b[0m     font \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetfont()\n\u001b[0;32m    670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(font, ImageFont\u001b[39m.\u001b[39mFreeTypeFont):\n\u001b[1;32m--> 671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mOnly supported for TrueType fonts\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    672\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRGBA\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m embedded_color \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfontmode\n\u001b[0;32m    673\u001b[0m bbox \u001b[39m=\u001b[39m font\u001b[39m.\u001b[39mgetbbox(\n\u001b[0;32m    674\u001b[0m     text, mode, direction, features, language, stroke_width, anchor\n\u001b[0;32m    675\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Only supported for TrueType fonts"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "corpora1 = lyrics_data.loc[lyrics_data['artist'] == 'cher']\n",
    "corpora2 = lyrics_data.loc[lyrics_data['artist'] == 'robyn']\n",
    "corpora3 = twitter_data.loc[twitter_data['artist'] == 'cher']\n",
    "corpora4 = twitter_data.loc[twitter_data['artist'] == 'robyn']\n",
    "\n",
    "\n",
    "corp1 = count_words(corpora1)\n",
    "corp2 = count_words(corpora2)\n",
    "corp3 = count_words(corpora3)\n",
    "corp4 = count_words(corpora4)\n",
    "\n",
    "\n",
    "\n",
    "word_corp1 = wordcloud(corp1[\"freq\"])\n",
    "print(\"Word Cloud for Corpora 1 \\n\")\n",
    "print(word_corp1)\n",
    "\n",
    "\n",
    "word_corp2 = wordcloud(corp2)\n",
    "print(\"\\n Word Cloud for Corpora 2 \\n\")\n",
    "print(word_corp2)\n",
    "\n",
    "\n",
    "word_corp3 = wordcloud(corp3['freq'])\n",
    "print(\"\\n Word Cloud for Corpora 3 \\n\")\n",
    "print(word_corp3)\n",
    "\n",
    "\n",
    "word_corp4 = wordcloud(corp4['freq'])\n",
    "print(\"\\n Word Cloud for Corpora 4 \\n\")\n",
    "print(word_corp4)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b11a2e53",
   "metadata": {},
   "source": [
    "Q: What observations do you have about these (relatively straightforward) wordclouds? \n",
    "\n",
    "A: I tried executing this code; however, I keep getting an error. I have researched trying to figure out how to fix it but I found nothing. I made sure I had the updated library installed as well. I am still getting this error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da9cd09d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
